{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92223c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os, cv2, math, statistics, datetime, itertools\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import xarray as xr\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "\n",
    "#from datetime import datetime\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from pvlib import location\n",
    "from pvlib.irradiance import get_extra_radiation\n",
    "from pvlib.location import Location\n",
    "from plotly import subplots\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0f4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List containing the location name, Latitude/Longitude/Altitude of the surface stations and number which will be displayed on map\n",
    "locations = [['brb', -15.60083, -47.71306, 1023, 'Brasilia'],\n",
    "             ['cpa', -22.6896, -45.0062, 574, 'Cachoeira Paulista'],\n",
    "             ['ptr', -9.0689, -40.3197, 387, 'Petrolina'],\n",
    "             ['sms', -29.4428, -53.8231, 489, 'Sao Martinho da Serra']]\n",
    "\n",
    "# Setting first time step t_0, this is the time which will be used to kickstart the forecast\n",
    "t = \"1511\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d36a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the source files path, this is where all files with the .nc extension are located\n",
    "path = r\"C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\inteiros\\01\"\n",
    "os.chdir(path)\n",
    "\n",
    "# Finding the time difference between two consecutive frames\n",
    "files = []\n",
    "for file in os.listdir(path):\n",
    "    files.append(file)\n",
    "\n",
    "delta = pd.to_datetime(files[1][-10:-6], format='%H%M').minute - pd.to_datetime(files[0][-10:-6], format='%H%M').minute\n",
    "del files\n",
    "\n",
    "# Setting previous time step, two consecutive frames are necessary for the use of optical flow\n",
    "t_0 = pd.to_datetime(t, format='%H%M') - datetime.timedelta(minutes=delta)\n",
    "t_0 = str(t_0.hour) + str(t_0.minute)\n",
    "\n",
    "# Creating list to store every netcdf file in which optical flow will be applied\n",
    "goes_frames_of = []\n",
    "\n",
    "# Looping thorugh goes files, adding to the list\n",
    "for file in os.listdir(path):\n",
    "    if file[-10:-6] == t_0 or file[-10:-6] == t:\n",
    "        goes_frames_of.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "862ec268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through locations\n",
    "for location in locations:\n",
    "    \n",
    "    # Looping through every .nc file in the directory\n",
    "    for file in goes_frames_of:\n",
    "        \n",
    "        # Opening the dataset with xarray library\n",
    "        ncfile = file\n",
    "        ds = xr.open_dataset(ncfile)\n",
    "        \n",
    "        # Extracting the date from the file\n",
    "        date = str(ds.attrs['date_created'])\n",
    "    \n",
    "        # Selecting the variables\n",
    "        lon = ds['lon'].values\n",
    "        lat = ds['lat'].values\n",
    "\n",
    "        # Setting the value of k, this is the size of our box around the location\n",
    "        k = 2.0\n",
    "        \n",
    "        # Setting the latitude and latitude\n",
    "        lat_station = location[1]\n",
    "        lon_station = location[2]\n",
    "        lat_list = list(lat)\n",
    "        lon_list = list(lon)\n",
    "        \n",
    "        # Finding the closest point in the dataset that matches our location latitude and longitude\n",
    "        lat_start = lat_list.index(lat_list[min(range(len(lat_list)), key = lambda i: abs(lat_list[i]-(lat_station - k)))])\n",
    "        lat_end = lat_list.index(lat_list[min(range(len(lat_list)), key = lambda i: abs(lat_list[i]-(lat_station + k)))])\n",
    "        lon_start = lon_list.index(lon_list[min(range(len(lon_list)), key = lambda i: abs(lon_list[i]-(lon_station - k)))])\n",
    "        lon_end = lon_list.index(lon_list[min(range(len(lon_list)), key = lambda i: abs(lon_list[i]-(lon_station + k)))])\n",
    "        \n",
    "        # Setting the new .nc file with our specified dimensions\n",
    "        data = ds['CMI'][lat_start:lat_end, lon_start:lon_end]\n",
    "        \n",
    "        # Naming our file with the date and time\n",
    "        file = location[0] + '_' + date[0:4] + '_' + date[5:7] + '_' + date[8:10] + '_' + date[11:13] + date[14:16]\n",
    "        \n",
    "        # Saving the cropped NetCDF file\n",
    "        data.to_netcdf(path=fr\"C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\recortados\\{file}.nc\")\n",
    "        ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4649426",
   "metadata": {},
   "outputs": [],
   "source": [
    "cci = [['brb', [0.1054, 0.6152], [0.1990, 0.5511], [0.1206, 0.3708], [0.2270, 0.6498]],\n",
    "      ['cpa', [0.1098, 0.4775], [0.1197, 0.5825], [0.0841, 0.4041], [0.1873, 0.7200]],\n",
    "      ['ptr', [0.2035, 0.5848], [0.1337, 0.5902], [0.2171, 0.5740], [0.1883, 0.4949]],\n",
    "      ['sms', [0.1689, 0.7349], [0.2305, 0.6181], [0.1314, 0.4914], [0.1616, 0.6251]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751b385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brb 0.1054 0.6152\n",
      "cpa 0.1098 0.4775\n",
      "ptr 0.2035 0.5848\n",
      "sms 0.1689 0.7349\n"
     ]
    }
   ],
   "source": [
    "# Path where files will be saved\n",
    "save_cluster_path = r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\cluster'\n",
    "ds_path = r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\recortados'\n",
    "\n",
    "for values in cci:\n",
    "    location = values[0]\n",
    "    clear_index = values[1][0]\n",
    "    cloud_index = values[1][1]\n",
    "    print(location, clear_index, cloud_index)\n",
    "    \n",
    "    for file in os.listdir(ds_path):\n",
    "        if location in file:\n",
    "            ds_2 = xr.open_dataset(ds_path + '\\\\' + file)\n",
    "            file_name = file[0:-3] + '.png'\n",
    "            ds_cmi = ds_2.CMI.values\n",
    "\n",
    "            # Calculating CCI\n",
    "            c_eff = (ds_cmi - clear_index)/(cloud_index - clear_index)\n",
    "\n",
    "            # Replacing values \n",
    "            c_eff = np.where(c_eff < 0, 0, c_eff)\n",
    "            c_eff = np.where(c_eff > 1, 1, c_eff)\n",
    "            ds_2.close()\n",
    "\n",
    "            # Rotating the array so it matches our geographical location, converting the file to 8-bit grayscale image\n",
    "            im = Image.fromarray(np.uint8(cm.gist_earth(np.rot90(np.flip(c_eff.T), k=3))*255)).convert('L')\n",
    "            im.save(save_cluster_path + '\\\\' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19cb3dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\cluster'\n",
    "os.chdir(r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\cluster')\n",
    "persistence = []\n",
    "\n",
    "\n",
    "for location in locations:\n",
    "    values_persist = []\n",
    "    for image in os.listdir(path):\n",
    "        if t == image[-8:-4] and location[0] == image[0:3]:\n",
    "            for i in range(8):\n",
    "                values_persist.append(np.mean(mpimg.imread(image)[224:227, 224:227]))\n",
    "    persistence.append(values_persist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cb58159",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_folder = rf\"C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\cluster\"\n",
    "directory = \"xx_xx\"\n",
    "\n",
    "for location in locations:\n",
    "    \n",
    "    if location[0] == 'brb':\n",
    "        levels = 5\n",
    "        winsize = 25\n",
    "        iterations = 3\n",
    "        \n",
    "    elif location[0] == 'cpa':\n",
    "        levels = 5\n",
    "        winsize = 35\n",
    "        iterations = 7 \n",
    "        \n",
    "    elif location[0] == 'ptr':\n",
    "        levels = 5\n",
    "        winsize = 25\n",
    "        iterations = 5 \n",
    "        \n",
    "    else:\n",
    "        levels = 7\n",
    "        winsize = 35\n",
    "        iterations = 7 \n",
    "        \n",
    "    files = []\n",
    "    \n",
    "    for file in os.listdir(rf\"C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\cluster\"):\n",
    "        if location[0] in file:\n",
    "            files.append(file)\n",
    "            \n",
    "    for i in range(len(files)-1):\n",
    "\n",
    "        if directory != files[i][9:14]:\n",
    "            directory = files[i][9:14]\n",
    "            parent_dir = rf\"C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\previstos\\{location[0]}\"\n",
    "            output_folder = os.path.join(parent_dir, directory)\n",
    "\n",
    "            try:\n",
    "                os.mkdir(output_folder, 0o666)\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "\n",
    "            # Sequential images to be used for the optical flow\n",
    "            frames = [files[i], files[i+1]]\n",
    "\n",
    "            # Forecast window (how many future frames will be generated)\n",
    "            forecast_window = int((120/delta) - 1)\n",
    "\n",
    "            # Algorithm split into 2 loops, the first one goes through all available frames\n",
    "            # The second loop creates forecast images from the previous generated forecast\n",
    "\n",
    "            # Loop 1: available frames\n",
    "            for i in range (1,len(frames)):\n",
    "\n",
    "                # Setting file path\n",
    "                nome_frame1 = os.path.join(frame_folder, frames[i-1])\n",
    "                nome_frame2 = os.path.join(frame_folder,frames[i])\n",
    "                #nome_saida = os.path.join(output_folder, 'FLOWMAP_FRAME{0}-{1}.png'.format(int(i-1), int(i)))\n",
    "                nome_saida2 = os.path.join(output_folder, 'FORECAST_FRAME{0}.png'.format(int(i+1)))\n",
    "\n",
    "                # Reading the images\n",
    "                frame1 = cv2.imread(nome_frame1)\n",
    "                prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "                frame2 = cv2.imread(nome_frame2)\n",
    "                nxt = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "                # Preparing the flowmap array\n",
    "                hsv_mask = np.zeros_like(frame1) \n",
    "                hsv_mask[..., 1] = 255\n",
    "\n",
    "                # Applying Farneback Optical Flow\n",
    "                flow = cv2.calcOpticalFlowFarneback(prvs, nxt, None, 0.5, levels, winsize, iterations, 5, 1.2, 0) \n",
    "#mudar 31 pra 5-10 e 25 pra 15 e 30 e de 3 pra 5,7,9\n",
    "\n",
    "                # Obtaining the magnitude and angle of the displacement vector\n",
    "                mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1], angleInDegrees = True) \n",
    "\n",
    "                # Creating mask to be used on future frames\n",
    "                mag_i = mag.astype(int)\n",
    "                mag_i = np.where(mag_i == 0, 1, 0)\n",
    "\n",
    "                # Splitting the vectors u and v from the optical flow\n",
    "                v = flow[...,1]\n",
    "                u = flow[...,0]\n",
    "\n",
    "                ub = u\n",
    "                vb = v\n",
    "\n",
    "                # Saving the flowmap in 4 directions\n",
    "                down_mask = np.where((ang > 45) & (ang <=135), 90, 0)\n",
    "                up_mask = np.where((ang > 225) & (ang <=315), 270, 0)\n",
    "                left_mask = np.where((ang > 135) & (ang <=225), 180, 0)\n",
    "                right_mask = np.where((ang > 315) | (ang <=45), 0, 0)\n",
    "                fin_mask = down_mask + up_mask + left_mask + right_mask\n",
    "\n",
    "                # Saturating the top 1% of the magnitude to reduce the range of the values\n",
    "                cutoff = np.percentile(mag, 99)\n",
    "                mag[mag>cutoff] = cutoff\n",
    "\n",
    "                # Filling the flowmap array\n",
    "                hsv_mask[..., 0] = fin_mask    \n",
    "                hsv_mask[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX) \n",
    "\n",
    "                # Converting the flowmap array to RGB and saving\n",
    "                rgb_representation = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR) \n",
    "                #cv2.imwrite(nome_saida, rgb_representation)\n",
    "\n",
    "\n",
    "                # Preparing the forecast frame. The multiplication by the magnitude mask allows the filling of the stationary areas,\n",
    "                # this avoids the overwriting of pixels\n",
    "                next_frame = frame2[:,:,0] * mag_i\n",
    "\n",
    "                # Conducting the forecast of the next frame\n",
    "                # It is necessary to go through each pixel, applying the dispplacement vector\n",
    "                # Pixels moving outside of the frame and stationary pixels are ignored\n",
    "                for i in range(frame2.shape[0]):\n",
    "                    for j in range(frame2.shape[1]):\n",
    "\n",
    "                        if (int(ub[i,j]==0) and int(vb[i,j]==0) ):\n",
    "                            continue\n",
    "\n",
    "                        if ((i + int(vb[i,j])) >= next_frame.shape[0]) or ((j + int(ub[i,j])) >= next_frame.shape[1]):\n",
    "                            continue\n",
    "\n",
    "                        next_frame[i + int(vb[i,j]), j + int(ub[i,j])] = frame2[i,j,0]\n",
    "\n",
    "\n",
    "                # Applying a selective median filter, this removes deformations\n",
    "                next_frame3 = cv2.medianBlur(next_frame.astype(np.float32), 5)    \n",
    "                next_frame2 = np.where(next_frame == 0, next_frame3, next_frame)\n",
    "\n",
    "                # Saving the forecasted frame and preparing the array for the next iteration\n",
    "                cv2.imwrite(nome_saida2, next_frame2.astype(np.int64))\n",
    "                next_frame = np.zeros( (next_frame2.shape[0],next_frame2.shape[1], 3) ).astype(int)\n",
    "                next_frame[...,0] = next_frame2.astype(int)\n",
    "                next_frame[...,1] = next_frame2.astype(int)\n",
    "                next_frame[...,2] = next_frame2.astype(int)\n",
    "\n",
    "            # Second loop: additional forecasts\n",
    "            # The amount of iterations is equal to the forecast range\n",
    "            for i in range(forecast_window):\n",
    "\n",
    "                # Setting file path\n",
    "                j = i + len(frames)\n",
    "                #nome_saida = os.path.join(output_folder, 'FLOWMAP_FRAME{0}-{1}.png'.format(int(j-1), int(j)))\n",
    "                nome_saida2 = os.path.join(output_folder, 'FORECAST_FRAME{0}.png'.format(int(j+1)))\n",
    "\n",
    "                # # Reading the images\n",
    "                frame1 = frame2.copy()\n",
    "                frame2 = next_frame.copy()    \n",
    "                prvs = frame1[...,0]\n",
    "                nxt = frame2[...,0]\n",
    "\n",
    "                # Preparing the flowmap array\n",
    "                hsv_mask = np.zeros((frame1.shape)).astype(np.uint8)\n",
    "                hsv_mask[..., 1] = 255\n",
    "\n",
    "                # Applying Farnerback Optical Flow\n",
    "                flow = cv2.calcOpticalFlowFarneback(prvs, nxt, None, 0.5, levels, winsize, iterations, 5, 1.2, 0) \n",
    "\n",
    "                # Obtaining the magnitude and angle of the displacement vector\n",
    "                mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1], angleInDegrees = True) \n",
    "\n",
    "                # Creating mask to be used on future frames\n",
    "                mag_i = mag.astype(int)\n",
    "                mag_i = np.where(mag_i == 0, 1, 0)\n",
    "\n",
    "                # Splitting the vectors u and v from the optical flow\n",
    "                v = flow[...,1]\n",
    "                u = flow[...,0]    \n",
    "\n",
    "                ub = u\n",
    "                vb = v\n",
    "\n",
    "                # 4 Directions\n",
    "                down_mask = np.where((ang > 45) & (ang <=135), 90, 0)\n",
    "                up_mask = np.where((ang > 225) & (ang <=315), 270, 0)\n",
    "                left_mask = np.where((ang > 135) & (ang <=225), 180, 0)\n",
    "                right_mask = np.where((ang > 315) | (ang <=45), 0, 0)\n",
    "                fin_mask = down_mask + up_mask + left_mask + right_mask\n",
    "\n",
    "                # Saturating the top 1% of the magnitude to reduce the range of the values\n",
    "                cutoff = np.percentile(mag, 99)\n",
    "                mag[mag>cutoff] = cutoff\n",
    "\n",
    "                # Filling the flowmap array\n",
    "                hsv_mask[..., 0] = fin_mask    \n",
    "                hsv_mask[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "                # Converting the flowmap array to RGB and saving\n",
    "                rgb_representation = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR) \n",
    "                #cv2.imwrite(nome_saida, rgb_representation)\n",
    "\n",
    "                # Preparing the forecast frame. The multiplication by the magnitude mask allows the filling of the stationary areas,\n",
    "                # this avoids the overwriting of pixels\n",
    "                next_frame = frame2[:,:,0] * mag_i\n",
    "\n",
    "                # Conducting the forecast of the next frame\n",
    "                # It is necessary to go through each pixel, applying the dispplacement vector\n",
    "                # Pixels moving outside of the frame and stationary pixels are ignored\n",
    "                for i in range(frame2.shape[0]):\n",
    "                    for j in range(frame2.shape[1]):\n",
    "\n",
    "                        if (int(ub[i,j]==0) and int(vb[i,j]==0) ):\n",
    "                            continue\n",
    "\n",
    "                        if ((i + int(vb[i,j])) >= next_frame.shape[0]) or ((j + int(ub[i,j])) >= next_frame.shape[1]):\n",
    "                            continue\n",
    "\n",
    "                        next_frame[i + int(vb[i,j]), j + int(ub[i,j])] = frame2[i,j,0]\n",
    "\n",
    "                # Applying a selective median filter, this removes deformations\n",
    "                next_frame3 = cv2.medianBlur(next_frame.astype(np.float32), 5)    \n",
    "                next_frame2 = np.where(next_frame == 0, next_frame3, next_frame)\n",
    "\n",
    "                # Saving the forecasted frame and preparing the array for the next iteration\n",
    "                cv2.imwrite(nome_saida2, next_frame2.astype(np.int64))\n",
    "                next_frame = np.zeros( (next_frame2.shape[0],next_frame2.shape[1], 3) ).astype(int)\n",
    "                next_frame[...,0] = next_frame2.astype(int)\n",
    "                next_frame[...,1] = next_frame2.astype(int)\n",
    "                next_frame[...,2] = next_frame2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c1b9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\previstos\\brb'\n",
    "os.chdir(path)\n",
    "timestamps = []\n",
    "\n",
    "for folder in os.listdir(path):\n",
    "    month = folder[0:2]\n",
    "    day = folder[3:5]\n",
    "    hour_start = str((pd.to_datetime(t, format='%H%M') + datetime.timedelta(minutes=delta)).hour) + ':' + str((pd.to_datetime(t, format='%H%M') + datetime.timedelta(minutes=delta)).minute)\n",
    "    hour_end = str((pd.to_datetime(t, format='%H%M') + datetime.timedelta(hours=2)).hour) + ':' + str((pd.to_datetime(t, format='%H%M') + datetime.timedelta(hours=2)).minute)\n",
    "    times = pd.date_range(start=f'2019-{month}-{day} {hour_start}', end=f'2019-{month}-{day} {hour_end}', freq=f'{delta}min')\n",
    "    timestamps.append(times)\n",
    "    \n",
    "index = list(itertools.chain(*timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66480d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\previstos'\n",
    "os.chdir(path)\n",
    "ceffs = []\n",
    "location_name = []\n",
    "\n",
    "for location in os.listdir(path):\n",
    "    \n",
    "    location_name.append(location)\n",
    "    location_ceff = []\n",
    "    new_path = path + rf'\\{location}'\n",
    "    \n",
    "    for folder in os.listdir(new_path):\n",
    "        new_path2 = new_path + rf'\\{folder}'\n",
    "        \n",
    "        for forecast in os.listdir(new_path2):\n",
    "            ceff_forecast = np.mean(mpimg.imread(new_path2 + rf'\\{forecast}')[224:227, 224:227])\n",
    "            location_ceff.append(ceff_forecast)\n",
    "            \n",
    "    ceffs.append(location_ceff)\n",
    "\n",
    "data = dict(zip(location_name, ceffs))\n",
    "df_forecast = pd.DataFrame(data, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa0cd1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extrarads = []\n",
    "\n",
    "for day in df_forecast.index.day:\n",
    "    date = datetime.datetime.strptime(f'01-{day}-2019', '%m-%d-%Y')\n",
    "    extrarads.append(get_extra_radiation(date, method='nrel'))\n",
    "\n",
    "location_data = []\n",
    "\n",
    "for location in locations:\n",
    "    data = Location(location[1], location[2], 'UTC', location[3], location[4])\n",
    "    location_data.append(data)\n",
    "\n",
    "ghi = location_data[0].get_clearsky(df_forecast.index).ghi/extrarads\n",
    "\n",
    "for column in df_forecast.columns:\n",
    "    df_forecast[column] = extrarads*((ghi - 0.05)*(1-df_forecast[column] + 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50dff60c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brb_persist</th>\n",
       "      <th>cpa_persist</th>\n",
       "      <th>ptr_persist</th>\n",
       "      <th>sms_persist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15:26:00</th>\n",
       "      <td>816.389045</td>\n",
       "      <td>695.830652</td>\n",
       "      <td>608.970060</td>\n",
       "      <td>459.333958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15:41:00</th>\n",
       "      <td>812.177581</td>\n",
       "      <td>692.370875</td>\n",
       "      <td>605.833500</td>\n",
       "      <td>456.958694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15:56:00</th>\n",
       "      <td>804.120345</td>\n",
       "      <td>685.629946</td>\n",
       "      <td>599.827976</td>\n",
       "      <td>452.419626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16:11:00</th>\n",
       "      <td>792.253561</td>\n",
       "      <td>675.638146</td>\n",
       "      <td>590.980488</td>\n",
       "      <td>445.737163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16:26:00</th>\n",
       "      <td>776.631048</td>\n",
       "      <td>662.440773</td>\n",
       "      <td>579.331158</td>\n",
       "      <td>436.941612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16:41:00</th>\n",
       "      <td>757.324023</td>\n",
       "      <td>646.097971</td>\n",
       "      <td>564.933092</td>\n",
       "      <td>426.073070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16:56:00</th>\n",
       "      <td>734.420939</td>\n",
       "      <td>626.684594</td>\n",
       "      <td>547.852254</td>\n",
       "      <td>413.181331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17:11:00</th>\n",
       "      <td>708.027363</td>\n",
       "      <td>604.290093</td>\n",
       "      <td>528.167372</td>\n",
       "      <td>398.325820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          brb_persist  cpa_persist  ptr_persist  sms_persist\n",
       "15:26:00   816.389045   695.830652   608.970060   459.333958\n",
       "15:41:00   812.177581   692.370875   605.833500   456.958694\n",
       "15:56:00   804.120345   685.629946   599.827976   452.419626\n",
       "16:11:00   792.253561   675.638146   590.980488   445.737163\n",
       "16:26:00   776.631048   662.440773   579.331158   436.941612\n",
       "16:41:00   757.324023   646.097971   564.933092   426.073070\n",
       "16:56:00   734.420939   626.684594   547.852254   413.181331\n",
       "17:11:00   708.027363   604.290093   528.167372   398.325820"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\cluster'\n",
    "os.chdir(r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\arquivos\\cluster')\n",
    "persistence = []\n",
    "\n",
    "\n",
    "for location in locations:\n",
    "    values_persist = []\n",
    "    for image in os.listdir(path):\n",
    "        if t == image[-8:-4] and location[0] == image[0:3]:\n",
    "            for i in range(8):\n",
    "                values_persist.append(np.mean(mpimg.imread(image)[224:227, 224:227]))\n",
    "    persistence.append(values_persist)\n",
    "\n",
    "test = {'brb_persist': persistence[0], 'cpa_persist': persistence[1], \n",
    "        'ptr_persist': persistence[2], 'sms_persist': persistence[3]}\n",
    "\n",
    "df_persist = pd.DataFrame(test, index = index)\n",
    "df_persist\n",
    "\n",
    "extrarads = []\n",
    "\n",
    "for day in df_persist.index.day:\n",
    "    date = datetime.datetime.strptime(f'01-{day}-2019', '%m-%d-%Y')\n",
    "    extrarads.append(get_extra_radiation(date, method='nrel'))\n",
    "\n",
    "location_data = []\n",
    "\n",
    "for location in locations:\n",
    "    data = Location(location[1], location[2], 'UTC', location[3], location[4])\n",
    "    location_data.append(data)\n",
    "\n",
    "ghi = location_data[0].get_clearsky(df_persist.index).ghi/extrarads\n",
    "\n",
    "for column in df_persist.columns:\n",
    "    df_persist[column] = extrarads*((ghi - 0.05)*(1-df_persist[column] + 0.05))\n",
    "\n",
    "df_persist = df_persist.groupby(df_persist.index.time).mean()\n",
    "df_persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa51fa89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brb</th>\n",
       "      <th>cpa</th>\n",
       "      <th>ptr</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-04 15:26:00</th>\n",
       "      <td>1085.216446</td>\n",
       "      <td>722.239170</td>\n",
       "      <td>1085.216446</td>\n",
       "      <td>889.316872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04 15:41:00</th>\n",
       "      <td>1071.983437</td>\n",
       "      <td>786.270468</td>\n",
       "      <td>1079.148570</td>\n",
       "      <td>884.344348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04 15:56:00</th>\n",
       "      <td>1067.979437</td>\n",
       "      <td>928.817515</td>\n",
       "      <td>1067.979437</td>\n",
       "      <td>875.191429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04 16:11:00</th>\n",
       "      <td>939.589190</td>\n",
       "      <td>598.713997</td>\n",
       "      <td>1051.759355</td>\n",
       "      <td>861.899341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04 16:26:00</th>\n",
       "      <td>1030.561944</td>\n",
       "      <td>588.357947</td>\n",
       "      <td>1030.561944</td>\n",
       "      <td>844.528414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11 16:11:00</th>\n",
       "      <td>66.172064</td>\n",
       "      <td>1043.280024</td>\n",
       "      <td>614.862510</td>\n",
       "      <td>626.714223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11 16:26:00</th>\n",
       "      <td>64.899456</td>\n",
       "      <td>1023.215870</td>\n",
       "      <td>603.037597</td>\n",
       "      <td>563.861163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11 16:41:00</th>\n",
       "      <td>63.318204</td>\n",
       "      <td>998.285591</td>\n",
       "      <td>588.344807</td>\n",
       "      <td>457.298175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11 16:56:00</th>\n",
       "      <td>61.435548</td>\n",
       "      <td>968.603315</td>\n",
       "      <td>570.851404</td>\n",
       "      <td>502.793487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11 17:11:00</th>\n",
       "      <td>355.266081</td>\n",
       "      <td>934.305742</td>\n",
       "      <td>550.637950</td>\n",
       "      <td>503.465685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             brb          cpa          ptr         sms\n",
       "2019-01-04 15:26:00  1085.216446   722.239170  1085.216446  889.316872\n",
       "2019-01-04 15:41:00  1071.983437   786.270468  1079.148570  884.344348\n",
       "2019-01-04 15:56:00  1067.979437   928.817515  1067.979437  875.191429\n",
       "2019-01-04 16:11:00   939.589190   598.713997  1051.759355  861.899341\n",
       "2019-01-04 16:26:00  1030.561944   588.357947  1030.561944  844.528414\n",
       "...                          ...          ...          ...         ...\n",
       "2019-01-11 16:11:00    66.172064  1043.280024   614.862510  626.714223\n",
       "2019-01-11 16:26:00    64.899456  1023.215870   603.037597  563.861163\n",
       "2019-01-11 16:41:00    63.318204   998.285591   588.344807  457.298175\n",
       "2019-01-11 16:56:00    61.435548   968.603315   570.851404  502.793487\n",
       "2019-01-11 17:11:00   355.266081   934.305742   550.637950  503.465685\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "566c76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = df_forecast.groupby(df_forecast.index.time).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c9634",
   "metadata": {},
   "source": [
    "## Dados para o Verão "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3a180d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\dados sonda validacao\\brb\\2019\\BRB_2019_001_a_031.csv', header=None, sep=';')\n",
    "df[1] = df[1].values.astype('str')\n",
    "df[2] = df[2].values.astype('str')\n",
    "\n",
    "df_time = []\n",
    "\n",
    "for minute in df[3]:\n",
    "    df_time.append(str(datetime.timedelta(minutes=minute)))\n",
    "\n",
    "df[3] = df_time\n",
    "\n",
    "month = []\n",
    "days = []\n",
    "\n",
    "for day in df[2]:\n",
    "    month.append(str(datetime.datetime.strptime(day, '%j').date().month))\n",
    "    days.append(str(datetime.datetime.strptime(day, '%j').date().day))\n",
    "\n",
    "df.index = pd.to_datetime(df[1].values + '-' + month + '-' + days + ' ' + df[3])\n",
    "df.drop(columns=[0, 1, 2, 3], inplace=True)\n",
    "df = df.groupby(df.index.time).mean()\n",
    "df = df.rolling(10).mean()\n",
    "\n",
    "mask1 = df.index >= df_forecast.index[0]\n",
    "mask2 = df.index <= df_forecast.index[-1]\n",
    "\n",
    "df = df.loc[mask1 & mask2]\n",
    "df_brb = df.iloc[::15, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd2608c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\dados sonda validacao\\cpa\\2019\\CPA_2019_001_a_031.dat', header=None)\n",
    "df.index = pd.to_datetime(df[0])\n",
    "df = df[6]\n",
    "df = df.groupby(df.index.time).mean()\n",
    "df = df.rolling(10).mean()\n",
    "\n",
    "mask1 = df.index >= df_forecast.index[0]\n",
    "mask2 = df.index <= df_forecast.index[-1]\n",
    "\n",
    "df = df.loc[mask1 & mask2]\n",
    "df_cpa = df.iloc[::15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbf98b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\dados sonda validacao\\ptr\\2019\\PTR_2019_001_a_031.dat', header=None)\n",
    "df = df.loc[df[0] == 111]\n",
    "df[1] = pd.to_numeric(df[1], downcast='integer')\n",
    "df[1] = df[1].values.astype('str')\n",
    "df[2] = df[2].values.astype('str')\n",
    "\n",
    "df_time = []\n",
    "\n",
    "for minute in df[3]:\n",
    "    df_time.append(str(datetime.timedelta(minutes=minute)))\n",
    "\n",
    "df[3] = df_time\n",
    "\n",
    "month = []\n",
    "days = []\n",
    "\n",
    "for day in df[2]:\n",
    "    month.append(str(datetime.datetime.strptime(day, '%j').date().month))\n",
    "    days.append(str(datetime.datetime.strptime(day, '%j').date().day))\n",
    "\n",
    "df.index = pd.to_datetime(df[1].values + '-' + month + '-' + days + ' ' + df[3])\n",
    "df.drop(columns=[0, 1, 2, 3], inplace=True)\n",
    "df = df.groupby(df.index.time).mean()\n",
    "df = df.rolling(10).mean()\n",
    "\n",
    "mask1 = df.index >= df_forecast.index[0]\n",
    "mask2 = df.index <= df_forecast.index[-1]\n",
    "\n",
    "df = df.loc[mask1 & mask2]\n",
    "df = df.iloc[::15, :]\n",
    "df_ptr = df[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21e9f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Usuario\\Documents\\Unifesp\\Mestrado\\dados sonda validacao\\sms\\2019\\SMS_2019_001_a_031.dat', header=None)\n",
    "df = df.loc[df[0] == 108]\n",
    "df[1] = pd.to_numeric(df[1], downcast='integer')\n",
    "df[1] = df[1].values.astype('str')\n",
    "df[2] = df[2].values.astype('str')\n",
    "\n",
    "df_time = []\n",
    "\n",
    "for minute in df[3]:\n",
    "    df_time.append(str(datetime.timedelta(minutes=minute)))\n",
    "\n",
    "df[3] = df_time\n",
    "\n",
    "month = []\n",
    "days = []\n",
    "\n",
    "for day in df[2]:\n",
    "    month.append(str(datetime.datetime.strptime(day, '%j').date().month))\n",
    "    days.append(str(datetime.datetime.strptime(day, '%j').date().day))\n",
    "\n",
    "df.index = pd.to_datetime(df[1].values + '-' + month + '-' + days + ' ' + df[3])\n",
    "df.drop(columns=[0, 1, 2, 3], inplace=True)\n",
    "df = df.groupby(df.index.time).mean()\n",
    "df = df.rolling(10).mean()\n",
    "\n",
    "mask1 = df.index >= df_forecast.index[0]\n",
    "mask2 = df.index <= df_forecast.index[-1]\n",
    "\n",
    "df = df.loc[mask1 & mask2]\n",
    "df = df.iloc[::15, :]\n",
    "df_sms = df[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85e28504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace0 = go.Scatter(x=[i for i in range(1,9)], y=df_brb[4],\n",
    "                    mode='lines+markers', name='Observado', line=dict(color=\"blue\") )\n",
    "\n",
    "trace1 = go.Scatter(x=[i for i in range(1,9)], y=df_forecast['brb'],\n",
    "                    mode='lines+markers', name='Previsto', line=dict(color=\"red\"))\n",
    "\n",
    "trace2 = go.Scatter(x=[i for i in range(1,9)], y=df_persist['brb_persist'],\n",
    "                    mode='lines+markers', name='Persistência', line=dict(color=\"gray\"))\n",
    "\n",
    "trace3 = go.Scatter(x=[i for i in range(1,9)], y=df_cpa,\n",
    "                    mode='lines+markers', name='Observado',line=dict(color=\"blue\"),showlegend=False)\n",
    "\n",
    "trace4 = go.Scatter(x=[i for i in range(1,9)], y=df_forecast['cpa'],\n",
    "                    mode='lines+markers', name='Previsto',line=dict(color=\"red\"),showlegend=False)\n",
    "\n",
    "trace5 = go.Scatter(x=[i for i in range(1,9)], y=df_persist['cpa_persist'],\n",
    "                    mode='lines+markers', name='Persistência', line=dict(color=\"gray\"),showlegend=False)\n",
    "\n",
    "trace6 = go.Scatter(x=[i for i in range(1,9)], y=df_ptr,\n",
    "                     mode='lines+markers', name='Observado',line=dict(color=\"blue\"),showlegend=False)\n",
    "\n",
    "trace7 = go.Scatter(x=[i for i in range(1,9)], y=df_forecast['ptr'],\n",
    "                     mode='lines+markers', name='Previsto',line=dict(color=\"red\"),showlegend=False)\n",
    "\n",
    "trace8 = go.Scatter(x=[i for i in range(1,9)], y=df_persist['ptr_persist'],\n",
    "                    mode='lines+markers', name='Persistência', line=dict(color=\"gray\"),showlegend=False)\n",
    "\n",
    "trace9 = go.Scatter(x=[i for i in range(1,9)], y=df_sms,\n",
    "                     mode='lines+markers', name='Observado',line=dict(color=\"blue\"),showlegend=False)\n",
    "\n",
    "trace10 = go.Scatter(x=[i for i in range(1,9)], y=df_forecast['sms'],\n",
    "                     mode='lines+markers', name='Previsto',line=dict(color=\"red\"),showlegend=False)\n",
    "\n",
    "trace11 = go.Scatter(x=[i for i in range(1,9)], y=df_persist['sms_persist'],\n",
    "                    mode='lines+markers', name='Persistência', line=dict(color=\"gray\"),showlegend=False)\n",
    "\n",
    "fig = subplots.make_subplots(rows=2, cols=2,\n",
    "                             subplot_titles=['brb', 'cpa', 'ptr', 'sms'],\n",
    "                             x_title='Previsão',\n",
    "                             y_title='Radiação Global W/m²',\n",
    "                             shared_yaxes=False,\n",
    "                             shared_xaxes=True,\n",
    "                             vertical_spacing=0.1,\n",
    "                             horizontal_spacing=0.01)\n",
    "\n",
    "fig.append_trace(trace0,1,1)\n",
    "fig.append_trace(trace1,1,1)\n",
    "fig.append_trace(trace2,1,1)\n",
    "fig.append_trace(trace3,1,2)\n",
    "fig.append_trace(trace4,1,2)\n",
    "fig.append_trace(trace5,1,2)\n",
    "fig.append_trace(trace6,2,1)\n",
    "fig.append_trace(trace7,2,1)\n",
    "fig.append_trace(trace8,2,1)\n",
    "fig.append_trace(trace9,2,2)\n",
    "fig.append_trace(trace10,2,2)\n",
    "fig.append_trace(trace11,2,2)\n",
    "\n",
    "fig.update_layout(width=1200, height=600, hovermode=\"x\", template='simple_white',\n",
    "                  title={'text': 'Previsão de 120 minutos 15:11 UTC'})\n",
    "pyo.plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brb_nrmse = str((np.sqrt((sum((df_forecast['brb'] - df_brb[4])**2))/len(df_forecast['brb'])))/np.mean(df_forecast['brb']))\n",
    "#cpa_nrmse = str((np.sqrt((sum((df_forecast['cpa'] - df_cpa)**2))/len(df_forecast['cpa'])))/np.mean(df_forecast['cpa']))\n",
    "#ptr_nrmse = str((np.sqrt((sum((df_forecast['ptr'] - df_ptr)**2))/len(df_forecast['ptr'])))/np.mean(df_forecast['ptr']))\n",
    "#sms_nrmse = str((np.sqrt((sum((df_forecast['sms'] - df_sms)**2))/len(df_forecast['sms'])))/np.mean(df_forecast['sms']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brb_rmse = str((np.sqrt((sum((df_forecast['brb'] - df_brb[4])**2))/len(df_forecast['brb']))))\n",
    "#cpa_rmse = str((np.sqrt((sum((df_forecast['cpa'] - df_cpa)**2))/len(df_forecast['cpa']))))\n",
    "#ptr_rmse = str((np.sqrt((sum((df_forecast['ptr'] - df_ptr)**2))/len(df_forecast['ptr']))))\n",
    "#sms_rmse = str((np.sqrt((sum((df_forecast['sms'] - df_sms)**2))/len(df_forecast['sms']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rmse = pd.read_csv(r'C:\\Users\\Usuario\\Desktop\\1511.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b266a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_rmse['brb_RMSE'][run] = brb_rmse.replace('.',',')\n",
    "#df_rmse['cpa_RMSE'][run] = cpa_rmse.replace('.',',')\n",
    "#df_rmse['ptr_RMSE'][run] = ptr_rmse.replace('.',',')\n",
    "#df_rmse['sms_RMSE'][run] = sms_rmse.replace('.',',')\n",
    "\n",
    "#df_rmse['brb_NRMSE'][run] = brb_nrmse.replace('.',',')\n",
    "#df_rmse['cpa_NRMSE'][run] = cpa_nrmse.replace('.',',')\n",
    "#df_rmse['ptr_NRMSE'][run] = ptr_nrmse.replace('.',',')\n",
    "#df_rmse['sms_NRMSE'][run] = sms_nrmse.replace('.',',')\n",
    "\n",
    "#df_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e67668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rmse.to_csv(r'C:\\Users\\Usuario\\Desktop\\1511.csv', sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67453444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7f7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
